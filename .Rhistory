train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(SAheart)
modelFit <- train(chd~age+alcohol+obesity+tobacco+
typea+ldl, method="glm",
family="binomial",data=trainSA)
modelFit <- train(factor(chd) ~ age+alcohol+obesity+tobacco+
typea+ldl, method="glm",
family="binomial",data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd,
predict(modelFit,testSA))
modelFit <- train(factor(chd) ~ age+alcohol+obesity+tobacco+
typea+ldl, method="glm",
family="binomial",data=trainSA)
modelFit <- train(chd~age+alcohol+obesity+tobacco+
typea+ldl, method="glm",
family="binomial",data=trainSA)
missClass(testSA$chd,
predict(modelFit,testSA))
missClass(trainSA$chd,
predict(modelFit,trainSA))
data(vowel.train)
data(vowel.test)
set.seed(33833)
?randomForest
library(randomForest)
?randomForest
randomForest(data=vowel.train,factor(y)~.)
modelFit <- randomForest(data=vowel.train,factor(y)~.)
varImp(modelFit)
library(devtools)
install_github(repo="riv",username="tomasgreif")
library(riv)
install.packages("riv")
x <- german_data
x$gbbin <- NULL
model <- rpart(data=x,formula=gb~.)
library(caret)
library(riv)
x <- german_data
x$gbbin <- NULL
model <- rpart(data=x,formula=gb~.)
library(rpart)
x <- german_data
x$gbbin <- NULL
model <- rpart(data=x,formula=gb~.)
data("GermanCredit")
x <- german_data
x <- GermanCredit
names(x)
head(GermanCredit$Class)
model <- train(Class~.,method="rpart",data=x)
model$yLimits
model$levels
parse_tree(x,model)
library(riv)
?riv
parse_tree <- function (df=NULL, model=NULL) {
log <- capture.output({
rpart.rules <- path.rpart(model,rownames(model$frame)[model$frame$var=="<leaf>"])
})
args <- c("<=",">=","<",">","=")
rules_out <- "case "
i <- 1
for (rule in rpart.rules) {
rule_out <- character(0)
for (component in rule) {
sep <- lapply(args, function(x) length(unlist(strsplit(component,x)))) > 1
elements <- unlist(strsplit(component,(args[sep])[1]))
if(!(elements[1]=="root")) {
if (is.numeric(df[,elements[[1]]])) {
rule_out <- c(rule_out,paste(elements[1],(args[sep])[1],elements[2]))
} else {
rule_out <- c(rule_out,paste0(elements[1]," in (",paste0("'",unlist(strsplit(elements[2],",")),"'",collapse=","),")"))
}
}
}
rules_out <- c(rules_out, paste0("when ", paste(rule_out,collapse=" AND ")," then 'node_" ,names(rpart.rules)[i],"'"))
if(i==length(rpart.rules)) rules_out <- c(rules_out," end ")
i <- i +1
}
sql_out <- paste(rules_out, collapse=" ")
sql_out
}
parse_tree(x,model)
model <- rpart(Class~.,data=x)
parse_tree(x,model)
model$splits
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
factor(vowel.train$y)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
modFitRF <- train(y~.,method="rf",data=vowel.train)
library(caret)
modFitRF <- train(y~.,method="rf",data=vowel.train)
modFitB <- train(y~.,method="gbm",data="vowel.train")
modFitB <- train(y~.,method="gbm",data=vowel.train)
predRF <- predict(modFitRF,vowel.test)
predB <- predict(modFitB, vowel.test)
confusionMatrix(predRF, vowel.test$y)
confusionMatrix(predB, vowel.test$y)
agreement <- (predB==predRF)
head(predB)
head(agreement)
predBagree[agreement]
predBagree <- predB[agreement]
head(predBagree)
confusionMatrix(predBagree,vowel.test$y)
confusionMatrix(predBagree,vowel.test$y[agreement])
set.seed(62433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainRF <- train(diagnosis~.,method='rf',data=training)
trainB <- train(diagnosis~.,method='gbm',data=training)
trainLDA <- train(diagnosis~.,method='lda',data=training)
predRF <- predict(trainRF,testing)
predB <- predict(trainB,testing)
predLDA <- predict(trainLDA, testing)
newDF <- data.frame(predRF,predB,predLDA,adData$diagnosis)
newDF <- data.frame(predRF,predB,predLDA,testing$diagnosis)
modelFit <- train(diagnosis~.,method='rf',data=newDF)
predictions <- predict(modelFit,testing)
confusionMatrix(predictions,testing$diagnosis)
dim(newDF)
head(newDF)
tail(newDF)
modelFit <- train(testing.diagnosis~.,method='rf',data=newDF)
predictions <- predict(modelFit,testing)
confusionMatrix(predictions,testing$diagnosis)
newDF <- data.frame(predRF,predB,predLDA,training$diagnosis)
modelFit <- train(testing.diagnosis~.,method='rf',data=newDF)
predictions <- predict(modelFit,testing)
confusionMatrix(predictions,testing$diagnosis)
confusionMatrix(predRF,testing$diagnosis)
confusionMatrix(predB,testing$diagnosis)
confusionMatrix(predLDA,testing$diagnosis)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
modelFit <- train(CompressiveStrength~.,
method='lasso',data=training)
varImp(modelFit)
?plot.enet
modelFit$finalModel
plot.enet(modelFit)
?enet
plot.enet(modelFit$results)
plot.enet(modelFit$bestTune)
plot.enet(modelFit$bestTune$fraction)
plot.enet(modelFit$finalModel$beta.pure)
modelFit$finalModel
modelFit$finalModel$lambda
plot.enet(enet(
x=subset(training, select=-c(CompressionStrength)),
y=subset(training,select=c(CompressionStrength))))
plot.enet(enet(
x=subset(training, select=-c(CompressiveStrength)),
y=subset(training,select=c(CompressiveStrength))))
plot.enet(enet(
x=as.matrix(subset(training, select=-c(CompressiveStrength))),
y=as.matrix(subset(training,select=c(CompressiveStrength)))))
?plot.enet
plot.enet(enet(
x=as.matrix(subset(training, select=-c(CompressiveStrength))),
y=as.matrix(subset(training,select=c(CompressiveStrength)))),xvar=penalty)
plot.enet(enet(
x=as.matrix(subset(training, select=-c(CompressiveStrength))),
y=as.matrix(subset(training,select=c(CompressiveStrength)))),xvar="penalty")
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
install.packages("forecast")
library(forecast)
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("C:/Users/chjac/Desktop/gaData.csv")
library(lubridate)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
install.packages("forecast")
library(forecast)
?bats
modelFit <- bats(tstrain)
modelFit$errors
predictions <- predict(modelFit, testing)
fcast <- forecast(modelFit)
accuracy(fcast,modelFit)
accuracy(fcast)
fcast <- forecast(modelFit, testing)
fcast <- forecast(modelFit)
fcast
preds <- predict(fcast,testing)
accuracy(preds)
??interval
plot(fcast)
plot(preds)
plot(fcast)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
setseed(3523)
set.seed(3523)
modelFit <- train(CompressiveStrength~.,method='svm',
data=concrete)
modelFit <- train(CompressiveStrength~.,method='svmRadial',
data=concrete)
preds <- predict(modelFit,testing)
confusionMatrix(preds,testing$CompressiveStrength)
head(preds)
head(testing$CompressiveStrength)
sqrt(sum((preds-testing$CompressiveStrength)^2))
modelFit <- train(CompressiveStrength~.,method='svmLinear',
data=concrete)
preds <- predict(modelFit,testing)
sqrt(sum((preds-testing$CompressiveStrength)^2))
modelFit <- train(CompressiveStrength~.,method='svmLinearWeights2',
data=concrete)
preds <- predict(modelFit,testing)
sqrt(sum((preds-testing$CompressiveStrength)^2))
library(e1071)
svm(CompressiveStrength~.,data=training)
modelFit <- svm(CompressiveStrength~.,data=training)
preds <- predict(modelFit,testing)
sqrt(sum((preds-testing$CompressiveStrength)^2))
accuracy(preds,testing$CompressiveStrength)
install.packages('leaflett')
install.packages('leaflet')
library(leaflet)
?leaflet
library(dplyr)
my_map <- leaflet() %>% addTiles()
my_map
demo()
"need" %>% c("beer")
library(dplyr)
"need" %>% c("beer")
print(c("need" %>% c("beer")))
c("need" %>% c("beer")) -> x
x
?paste
c("need" %>% c("a","beer")) -> x %>% paste("I")
c("need" %>% c("beer")) -> x
x
c("need" %>% c("beer")) -> x; x %>% paste("I")
c("need" %>% c("beer")) -> x; x %>% paste(c("a","I"))
c("I" %>% c("a")) -> x; x %>% paste(c("need","beer"))
c("I" %>% c("a")) -> x; x %>% paste(c("need","beer"))
3 %>% '+'(4)
install.packages('shiny')
library(shiny)
shinyUI(fluidPage(
# Application title
titlePanel("Old Faithful Geyser Data"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("bins",
"Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
))
shinyServer(function(input, output) {
output$distPlot <- renderPlot({
# generate bins based on input$bins from ui.R
x    <- faithful[, 2]
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the histogram with the specified number of bins
hist(x, breaks = bins, col = 'darkgray', border = 'white')
})
})
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
faithful[,2]
faithful
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
library(GGally)
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
mean(c(1,2))
as.numeric("1, 2")
as.numeric(unlist(("1, 2"))
)
as.numeric(unlist(c("1, 2")))
c("1,2")
split(c("1,2"),",")
?split
read.csv(c("1,2"))
strsplit(c("1,2"))
strsplit(c("1,2"),"'")
strsplit(c("1,2"),"m")
strsplit(c("1,2"),",")
unlist(strsplit(c("1,2"),","))
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
strsplit(c(1,2,3),",")
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
meansIn <- "1,2,3,4,5,6,7,8,9"
class(meansIn)
means <- data.frame(strsplit(as.character(c(input$meansIn)),","))
means <- data.frame(strsplit(as.character(c(meansIn)),","))
means
dim(means)
means <- strsplit(as.character(c(meansIn)),",")
means
means <- as.numericstrsplit(as.character(c(meansIn)),",")
means <- as.numeric(strsplit(as.character(c(meansIn)),","))
means <- as.integer(strsplit(as.character(c(meansIn)),","))
class(menas)
class(means)
means$1
means[[1]]
unlist(means)
as.numeric(unlist(means))
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
do.call(rnorm,list(1,3,3))
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
?submitButton
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/CaffoBrushExample')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/CaffoBrushExample')
runApp('C:/Users/chjac/OneDrive/Coding/R/Developing Data Products/myTestApp')
install.packages("tidyverse")
library(tidyverse)
setwd("C:/Users/chjac/OneDrive/Coding/R/Gateway Exam/Shiny-App-with-Toydata")
library(reshape2)
library(tidyr)
library(dplyr)
###### Read in and mutuate data #####################################
toydata <- read.csv("RawData/toyAssessmentData.csv",
header = TRUE,
row.names = 1,
stringsAsFactors = FALSE)
## Let's group appropriate codes:
codesToKeep <- c("ie-e", "de-r", "ie-a", "ie-m", "ie-s")
codesToChange <- c("(ep)", "(ldm)", "(ade)", "(me)", "(se)")
#this data frame is not used but can be referenced
#to see which terms are grouped.
codeChanges <- cbind(codesToKeep,codesToChange)
#Change all error codes
for (i in 1:5){
toydata$errorcode1 <-
gsub(codesToChange[i],codesToKeep[i],
toydata$errorcode1)
toydata$errorcode2 <-
gsub(codesToChange[i],codesToKeep[i],
toydata$errorcode2)
}
allcode1s <- strsplit(toydata$errorcode1, ";")
allcode2s <- strsplit(toydata$errorcode2, ";")
allcode1and2s <- list(allcode1s, allcode2s)
coursenames <- unique(toydata$Course)
byTest <- function(data, testNum) {
coursecodes <- list()
## splitting codes by semicolon, ###########
## counting each & finding percentage ######
for(i in 1:7){
whichcourse <- data$Course==coursenames[i]
if(testNum == 0) {
code1 <- unlist(allcode1s[whichcourse])
code2 <- unlist(allcode2s[whichcourse])
counts <- table(c(code1, code2))
testcount <- 2 * nrow(data[whichcourse, ])
}
else {
codes <- unlist(allcode1and2s[[testNum]][whichcourse])
counts <- table(c(codes))
testcount <- nrow(data[whichcourse, ])
}
uncourse <- unique(data[whichcourse, 1:3])
studcount <- sum(uncourse$numberstudents)
## This could be problematic
## e.g count of any error coulbe > studcount because of 2 tests
percs <- counts/testcount
tograph <- data.frame(codes = names(counts),
counts = as.vector(counts),
percs = as.vector(percs),
studcount = studcount,
testcount = testcount)
coursecodes[[i]] <- tograph
}
names(coursecodes) <- coursenames
return(coursecodes)
}
test1 <- byTest(toydata[-c(6, 7)], 1)
test2 <- byTest(toydata[-c(4,5)], 2)
testAll <- byTest(toydata, 0)
courseresults = list()
for(i in 1:7) {
courseresults[[i]] = list(testAll[[i]], test1[[i]], test2[[i]])
}
names(courseresults) <- coursenames
names(testAll) <- coursenames
allcodes <- melt(testAll,
id.vars = c("codes", "counts", "percs", "studcount", "testcount"))
names(allcodes)[6] <- "course"
## creating labels for graph to show number of students per course
allcodes$label <- paste(allcodes$course,
paste(allcodes$studcount, "students,", allcodes$testcount, "tested"),
sep = " - ")
###### Means and Median per criteria##################################
c1mean <- tapply(toydata$score1, INDEX = toydata$Course, FUN = mean)
c1median <- tapply(toydata$score1, INDEX = toydata$Course, FUN = median)
c2mean <- tapply(toydata$score2, INDEX = toydata$Course, FUN = mean)
c2median <- tapply(toydata$score2, INDEX = toydata$Course, FUN = median)
allmean <- tapply(c(toydata$"score1", toydata$"score2"),
INDEX = rep(toydata$Course, 2),
FUN = mean)
allmedian <- tapply(c(toydata$"score1", toydata$"score2"),
INDEX = rep(toydata$Course, 2),
FUN = median)
meansmedsdf <- data.frame(c1mean, c1median,
c2mean, c2median,
allmean, allmedian)
###### Percent and frequency abovee 2.5 #############################
# here we pipe the data rather than using tapply/do.call
# since we want the high scores *per test* and overall
temp <- toydata %>% group_by(Course) %>% summarize(n = n())
highscores1 <- toydata %>% group_by(Course) %>%
mutate(numberStudents1 = n()) %>%
filter(score1>2.5) %>%
mutate(freq2.5Test1 = n(),
perc2.5Test1 = freq2.5Test1/numberStudents1) %>%
summarize(freq2.5Test1 = first(freq2.5Test1),
perc2.5Test1 = first(perc2.5Test1),
numberStudents1 = first(numberStudents1))
highscores1.5 <- highscores1[,c("Course","freq2.5Test1","perc2.5Test1",
"numberStudents1")]
highscores2 <- toydata %>% group_by(Course) %>%
mutate(numberStudents2 = n()) %>%
filter(score2>2.5) %>%
mutate(freq2.5Test2 = n(),
perc2.5Test2 = freq2.5Test2/numberStudents2) %>%
summarize(freq2.5Test2 = first(freq2.5Test2),
perc2.5Test2 = first(perc2.5Test2),
numberStudents2 = first(numberStudents2))
highscores2.5 <- highscores2[,c("Course","freq2.5Test2","perc2.5Test2",
"numberStudents2")]
highscoresnew <- cbind(highscores1[,c(2,3,4)], highscores2[,c(2,3,4)]) %>%
mutate(highScoresAllFreq = freq2.5Test1+freq2.5Test2,
highScoresAllPerc = highScoresAllFreq/
(numberStudents1+numberStudents2))
highscores <- tapply(c(toydata$"score1", toydata$"score2"),
INDEX = rep(toydata$Course, 2),
FUN = function(x) {y <- table(x>2.5);
data.frame(freqAbove2.5 = y[2],
percAbove2.5 = 100*y[2]/sum(y))})
# do.call() with rbind takes the elements of
# highscores and rbinds them into one data frame
highscoresdf <- do.call(rbind, highscores)
###### Results table ################################################
results <- merge(meansmedsdf, highscoresdf, by = "row.names")
#lets put together the high scores for both tests.
results <- cbind(results, highscoresnew)
countCodes <- allcodes %>% group_by(course) %>%
summarize(freqErrorPerCourse = sum(counts))
results <- cbind(results, countCodes)
# this code below gives us 3 columns for
names(results)[1] <- "course"
library(shiny); runApp('ScoresApp.R')
resulst
results
runApp('ScoresApp.R')
